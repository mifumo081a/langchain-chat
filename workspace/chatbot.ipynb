{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "IKxLCye71wl8"
   },
   "source": [
    "- https://huggingface.co/docs/transformers/model_doc/gpt_neox_japanese\n",
    "- https://tech.nri-net.com/entry/tried_langchain_to_extend_chatgpt\n",
    "- https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads\n",
    "- https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n",
    "- https://zenn.dev/umi_mori/books/prompt-engineer/viewer/chatgpt_webapp_langchain_streamlit\n",
    "- https://zenn.dev/umi_mori/books/prompt-engineer/viewer/langchain_memory\n",
    "- https://ainow.ai/2022/08/30/267101/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mifumo081a/langchain-chat/blob/master/workspace/chatbot.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5815,
     "status": "ok",
     "timestamp": 1686712858361,
     "user": {
      "displayName": "kouhei fujiwara",
      "userId": "03858348519444108736"
     },
     "user_tz": -540
    },
    "id": "ZVa7XRAZ1qGL",
    "outputId": "9486995f-ffea-40df-cf13-04eb6477193d",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install transformers sentencepiece accelerate langchain openai python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "from langchain import PromptTemplate, LLMChain, HuggingFacePipeline\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.schema import AIMessage\n",
    "\n",
    "\n",
    "chatgpt_id = \"gpt-3.5-turbo\"\n",
    "en_list = [\n",
    "        \"bigscience/bloom-560m\",\n",
    "        \"bigscience/bloom-1b7\",\n",
    "        \"bigscience/bloomz-560m\",\n",
    "        \"bigscience/bloomz-1b7\",\n",
    "        # \"gpt2\",\n",
    "        \"gpt2-medium\",\n",
    "        \"gpt2-large\",\n",
    "        \"gpt2-xl\",\n",
    "        # \"facebook/opt-125m\",\n",
    "        \"facebook/opt-350m\",\n",
    "        \"facebook/opt-1.3b\",\n",
    "        # \"cerebras/Cerebras-GPT-111M\",\n",
    "        \"cerebras/Cerebras-GPT-256M\",\n",
    "        \"cerebras/Cerebras-GPT-590M\",\n",
    "        \"cerebras/Cerebras-GPT-1.3B\",\n",
    "        \"vicgalle/gpt2-alpaca\",\n",
    "]\n",
    "ja_list = [\n",
    "        # \"cyberagent/open-calm-small\",\n",
    "        \"cyberagent/open-calm-medium\",\n",
    "        \"cyberagent/open-calm-large\",\n",
    "        \"cyberagent/open-calm-1b\",\n",
    "        # \"rinna/japanese-gpt2-xsmall\",\n",
    "        # \"rinna/japanese-gpt2-small\",\n",
    "        \"rinna/japanese-gpt2-medium\",\n",
    "        # \"rinna/japanese-gpt-1b\",\n",
    "        \"rinna/japanese-gpt-neox-small\",\n",
    "        \"abeja/gpt2-large-japanese\",\n",
    "        \"abeja/gpt-neox-japanese-2.7b\",\n",
    "]\n",
    "\n",
    "\n",
    "def get_llm(model_id, model_kwargs, pipeline_kwargs):\n",
    "    if torch.cuda.is_available():\n",
    "        device = 0\n",
    "    else:\n",
    "        device = -1\n",
    "    if model_id == chatgpt_id:\n",
    "        load_dotenv()\n",
    "        llm = ChatOpenAI(model_name=chatgpt_id)\n",
    "    else:\n",
    "        llm = HuggingFacePipeline.from_model_id(\n",
    "            model_id, task=\"text-generation\",\n",
    "            model_kwargs=model_kwargs,\n",
    "            pipeline_kwargs=pipeline_kwargs,\n",
    "            device=device,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "    return llm\n",
    "\n",
    "def get_prompt(template):\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a498e9eb5a485a8c47eeac5bf7bed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf18d88964e463cbe62e72ba294e4e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5180e11ea8ff4699ab7ff52304001baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caaec112aa134087be4189ff0acf5cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/693 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3968a134e0dc46cfb545f9663af45b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "model_kwargs = {\n",
    "            \"min_length\": 20,\n",
    "            \"max_length\": 100,\n",
    "            \"repetition_penalty\": 1.01,\n",
    "            \"do_sample\": True,\n",
    "            \"top_p\": 0.95,\n",
    "            \"top_k\": 50,\n",
    "            \"temperature\": 0.1,\n",
    "        }\n",
    "pipeline_kwargs = {\n",
    "            \"min_new_tokens\": 5,\n",
    "            \"max_new_tokens\": 50,\n",
    "}\n",
    "\n",
    "model_id = \"bigscience/bloom-560m\"\n",
    "\n",
    "template = \"\"\"You are a chatbot having a conversation with a human.\n",
    "{chat_history}\n",
    "Human: {input}\n",
    "Chatbot: \"\"\"\n",
    "template = '''{chat_history}\n",
    "Human: {input}\n",
    "AI: Let's think step by step. '''\n",
    "\n",
    "llm = get_llm(model_id, model_kwargs, pipeline_kwargs)\n",
    "prompt = get_prompt(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Human: What is AI?\n",
      "AI: Let's think step by step. \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: What is AI?\n",
      "AI: \n",
      "(1) The first thing that we need to do, and the most important one in this process of learning a language from scratch (and not just for our own personal use), is to understand what it means. We can say without any doubt that\n",
      "Human: Tell me some more details.\n",
      "AI: Let's think step by step. \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Human: What is AI?\n",
      "AI: \n",
      "(1) The first thing that we need to do, and the most important one in this process of learning a language from scratch (and not just for our own personal use), is to understand what it means. We can say without any doubt that\n",
      "Human: Tell me some more details.\n",
      "AI: \n",
      "(2) In order to learn something new or change an existing knowledge base into another form, you have two choices; either you are going to try out all possible ways to solve your problem with different methods and/or strategies. (3) If there isn't enough\n",
      "[HumanMessage(content='What is AI?', additional_kwargs={}, example=False), AIMessage(content='\\n(1) The first thing that we need to do, and the most important one in this process of learning a language from scratch (and not just for our own personal use), is to understand what it means. We can say without any doubt that', additional_kwargs={}, example=False), HumanMessage(content='Tell me some more details.', additional_kwargs={}, example=False), AIMessage(content=\"\\n(2) In order to learn something new or change an existing knowledge base into another form, you have two choices; either you are going to try out all possible ways to solve your problem with different methods and/or strategies. (3) If there isn't enough\", additional_kwargs={}, example=False)]\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(#return_messages=True,\n",
    "                                        memory_key=\"chat_history\",\n",
    "                                        k=5\n",
    "                                       )\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    prompt=prompt,\n",
    "    memory = memory\n",
    ")\n",
    "\n",
    "\n",
    "chain(\"What is AI?\")\n",
    "chain(\"Tell me some more details.\")\n",
    "\n",
    "print(memory.load_memory_variables({})[\"chat_history\"])\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tell me some more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: What is AI?\n",
      "AI: \n",
      "(1) The first thing that we need to do, and the most important one in this process of learning a language from scratch (and not just for our own personal use), is to understand what it means. We can say without any doubt that\n",
      "Human: Tell me some more details.\n",
      "AI: \n",
      "(2) In order to learn something new or change an existing knowledge base into another form, you have two choices; either you are going to try out all possible ways to solve your problem with different methods and/or strategies. (3) If there isn't enough\n",
      "Human: Tell me some more details.\n",
      "AI: Let's think step by step. \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "(4) You will be able to make decisions based on these three possibilities.\n",
      "(5) This way you'll get better at solving problems because you're using only those things which already exist within yourself as well as other people who know about them. \n",
      "So if I\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Speach without ( and ).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: What is AI?\n",
      "AI: \n",
      "(1) The first thing that we need to do, and the most important one in this process of learning a language from scratch (and not just for our own personal use), is to understand what it means. We can say without any doubt that\n",
      "Human: Tell me some more details.\n",
      "AI: \n",
      "(2) In order to learn something new or change an existing knowledge base into another form, you have two choices; either you are going to try out all possible ways to solve your problem with different methods and/or strategies. (3) If there isn't enough\n",
      "Human: Tell me some more details.\n",
      "AI: \n",
      "(4) You will be able to make decisions based on these three possibilities.\n",
      "(5) This way you'll get better at solving problems because you're using only those things which already exist within yourself as well as other people who know about them. \n",
      "So if I\n",
      "Human: Speach without ( and ).\n",
      "AI: Let's think step by step. \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "(6)\n",
      "In fact, it's very simple -you don't even need much time when talking to someone else! \n",
      "And that's why many experts believe that human beings speak like animals!\n",
      "But how does humans communicate? Well, they talk through their brains instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " exit\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    in_text = input()\n",
    "    if in_text == \"exit\":\n",
    "        break\n",
    "    output = chain.predict(input=in_text)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNU64eF9F/X6PQt+Clolah3",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
